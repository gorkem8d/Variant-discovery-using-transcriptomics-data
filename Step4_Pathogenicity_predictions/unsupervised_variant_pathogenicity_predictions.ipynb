{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09eea6de-d2f8-4c16-91d7-3eb912b6f1b3",
   "metadata": {},
   "source": [
    "## Unsupervised ML variant pathogenicity prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d0c30-4eeb-467c-8189-041dfbf8db6c",
   "metadata": {},
   "source": [
    "Part 1 - Common Mutations Between Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bcc6d3-940c-4b68-9eb7-db3a1e941702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67496ff9-b4cb-489f-814c-fa0d926f09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "result_df = pd.read_csv(\"result_df.csv\")\n",
    "Final_table_unfiltered= pd.read_csv(\"Final_table_unfiltered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4cfdd27-2f13-4ca1-9d86-20471bed53e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Datasets</th>\n",
       "      <th>Healthy</th>\n",
       "      <th>SLE</th>\n",
       "      <th>Occurrence</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>GQ</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DP</th>\n",
       "      <th>...</th>\n",
       "      <th>FILTER</th>\n",
       "      <th>IMPACT</th>\n",
       "      <th>BIOTYPE</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>AF</th>\n",
       "      <th>Amino_acids</th>\n",
       "      <th>CADD_PHRED</th>\n",
       "      <th>SIFT</th>\n",
       "      <th>PolyPhen</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68003029</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>875.306667</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>missense_variant,missense_variant,NMD_transcri...</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>L129I</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.951</td>\n",
       "      <td>85.855719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67999234</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1490.640000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>missense_variant,3_prime_UTR_variant,NMD_trans...</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>P209L</td>\n",
       "      <td>25.400</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.996</td>\n",
       "      <td>71.711330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67991568</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>968.640000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>protein_coding,retained_intron</td>\n",
       "      <td>missense_variant,downstream_gene_variant</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>G591E</td>\n",
       "      <td>17.490</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.995</td>\n",
       "      <td>64.518414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67999680</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>42.640000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>missense_variant,splice_region_variant,missens...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>R131S</td>\n",
       "      <td>25.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.350</td>\n",
       "      <td>57.554599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67997719</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>346.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODERATE</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>missense_variant,downstream_gene_variant,upstr...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>V288L</td>\n",
       "      <td>26.500</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.469</td>\n",
       "      <td>51.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>67995404</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>33.857143</td>\n",
       "      <td>64.594762</td>\n",
       "      <td>4.428571</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODIFIER</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>intron_variant,downstream_gene_variant,intron_...</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>-</td>\n",
       "      <td>5.551</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.298182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>67994228</td>\n",
       "      <td>1,3</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>35</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>20.914286</td>\n",
       "      <td>84.730000</td>\n",
       "      <td>3.457143</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODIFIER</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>intron_variant,downstream_gene_variant,intron_...</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>-</td>\n",
       "      <td>1.069</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.231546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>68000750</td>\n",
       "      <td>2,3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>78.725000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODIFIER</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>intron_variant,intron_variant,NMD_transcript_v...</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>-</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.230818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>67999031</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>T</td>\n",
       "      <td>G</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>68.673333</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODIFIER</td>\n",
       "      <td>protein_coding,nonsense_mediated_decay,retaine...</td>\n",
       "      <td>intron_variant,intron_variant,NMD_transcript_v...</td>\n",
       "      <td>0.1595</td>\n",
       "      <td>-</td>\n",
       "      <td>3.338</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.164858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>67990341</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>22.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>PASS</td>\n",
       "      <td>MODIFIER</td>\n",
       "      <td>protein_coding,retained_intron</td>\n",
       "      <td>downstream_gene_variant,downstream_gene_variant</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>-</td>\n",
       "      <td>9.763</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.973863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>663 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          POS Datasets  Healthy  SLE  Occurrence REF ALT         GQ  \\\n",
       "0    68003029        2        0    3           3   G   T  99.000000   \n",
       "1    67999234        2        0    3           3   G   A  99.000000   \n",
       "2    67991568        2        0    1           1   C   T  99.000000   \n",
       "3    67999680        2        0    1           1   C   A  50.000000   \n",
       "4    67997719        3        0    1           1   C   A  99.000000   \n",
       "..        ...      ...      ...  ...         ...  ..  ..        ...   \n",
       "658  67995404    1,2,3        2   19          21   T   C  33.857143   \n",
       "659  67994228      1,3       13   22          35   A   C  20.914286   \n",
       "660  68000750      2,3        1    3           4   T   G  35.000000   \n",
       "661  67999031    1,2,3        3    3           6   T   G  26.000000   \n",
       "662  67990341        3        1    0           1   C   T   6.000000   \n",
       "\n",
       "            QUAL          DP  ...  FILTER    IMPACT  \\\n",
       "0     875.306667   65.000000  ...    PASS  MODERATE   \n",
       "1    1490.640000   72.000000  ...    PASS  MODERATE   \n",
       "2     968.640000   41.000000  ...    PASS  MODERATE   \n",
       "3      42.640000   50.000000  ...    PASS  MODERATE   \n",
       "4       0.000000  346.000000  ...    PASS  MODERATE   \n",
       "..           ...         ...  ...     ...       ...   \n",
       "658    64.594762    4.428571  ...    PASS  MODIFIER   \n",
       "659    84.730000    3.457143  ...    PASS  MODIFIER   \n",
       "660    78.725000    5.000000  ...    PASS  MODIFIER   \n",
       "661    68.673333    2.833333  ...    PASS  MODIFIER   \n",
       "662    22.300000    2.000000  ...    PASS  MODIFIER   \n",
       "\n",
       "                                               BIOTYPE  \\\n",
       "0    protein_coding,nonsense_mediated_decay,retaine...   \n",
       "1    protein_coding,nonsense_mediated_decay,retaine...   \n",
       "2                       protein_coding,retained_intron   \n",
       "3    protein_coding,nonsense_mediated_decay,retaine...   \n",
       "4    protein_coding,nonsense_mediated_decay,retaine...   \n",
       "..                                                 ...   \n",
       "658  protein_coding,nonsense_mediated_decay,retaine...   \n",
       "659  protein_coding,nonsense_mediated_decay,retaine...   \n",
       "660  protein_coding,nonsense_mediated_decay,retaine...   \n",
       "661  protein_coding,nonsense_mediated_decay,retaine...   \n",
       "662                     protein_coding,retained_intron   \n",
       "\n",
       "                                           Consequence      AF Amino_acids  \\\n",
       "0    missense_variant,missense_variant,NMD_transcri...  0.0028       L129I   \n",
       "1    missense_variant,3_prime_UTR_variant,NMD_trans...  0.0028       P209L   \n",
       "2             missense_variant,downstream_gene_variant  0.0024       G591E   \n",
       "3    missense_variant,splice_region_variant,missens...  0.0000       R131S   \n",
       "4    missense_variant,downstream_gene_variant,upstr...  0.0000       V288L   \n",
       "..                                                 ...     ...         ...   \n",
       "658  intron_variant,downstream_gene_variant,intron_...  0.2324           -   \n",
       "659  intron_variant,downstream_gene_variant,intron_...  0.5238           -   \n",
       "660  intron_variant,intron_variant,NMD_transcript_v...  0.0148           -   \n",
       "661  intron_variant,intron_variant,NMD_transcript_v...  0.1595           -   \n",
       "662    downstream_gene_variant,downstream_gene_variant  0.0272           -   \n",
       "\n",
       "    CADD_PHRED  SIFT  PolyPhen      score  \n",
       "0       26.000  0.02     0.951  85.855719  \n",
       "1       25.400  0.00     0.996  71.711330  \n",
       "2       17.490  0.48     0.995  64.518414  \n",
       "3       25.000  0.00     0.350  57.554599  \n",
       "4       26.500  0.05     0.469  51.914286  \n",
       "..         ...   ...       ...        ...  \n",
       "658      5.551  0.00     0.000   4.298182  \n",
       "659      1.069  0.00     0.000   4.231546  \n",
       "660      0.154  0.00     0.000   4.230818  \n",
       "661      3.338  0.00     0.000   4.164858  \n",
       "662      9.763  0.00     0.000   3.973863  \n",
       "\n",
       "[663 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataf = Final_table_unfiltered.drop(\"#Uploaded_variation\", axis=1)\n",
    "dataf = dataf.drop(\"Protein_position\", axis=1)\n",
    "dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "992bc525-5a59-4b1b-9b88-4ed4de2a9b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top contributing features to variant scoring:\n",
      "  - CADD: 0.338\n",
      "  - Base_consequence: 0.331\n",
      "  - Pathogenicity_composite: 0.330\n",
      "  - AA_change_factor: 0.329\n",
      "  - Impact: 0.326\n",
      "\n",
      "ML Score Distribution:\n",
      "Very high risk (85-100): 113 variants (17.0%)\n",
      "High risk (70-84): 158 variants (23.8%)\n",
      "Moderate-high risk (55-69): 75 variants (11.3%)\n",
      "Moderate risk (40-54): 151 variants (22.8%)\n",
      "Low-moderate risk (25-39): 99 variants (14.9%)\n",
      "Low risk (10-24): 39 variants (5.9%)\n",
      "Very low risk (0-9): 28 variants (4.2%)\n"
     ]
    }
   ],
   "source": [
    "# Define weights\n",
    "# Original weights dictionary remains the same\n",
    "weights = {\n",
    "    'intergenic_variant': 1,\n",
    "    'feature_truncation': 3,\n",
    "    'regulatory_region_variant': 3,\n",
    "    'feature_elongation': 3,\n",
    "    'regulatory_region_amplification': 3,\n",
    "    'regulatory_region_ablation': 3,\n",
    "    'TF_binding_site_variant': 3,\n",
    "    'TFBS_amplification': 3,\n",
    "    'TFBS_ablation': 3,\n",
    "    'downstream_gene_variant': 3,\n",
    "    'upstream_gene_variant': 3,\n",
    "    'non_coding_transcript_variant': 3,\n",
    "    'NMD_transcript_variant': 3,\n",
    "    'intron_variant': 3,\n",
    "    'non_coding_transcript_exon_variant': 3,\n",
    "    '3_prime_UTR_variant': 5,\n",
    "    '5_prime_UTR_variant': 5,\n",
    "    'mature_miRNA_variant': 5,\n",
    "    'coding_sequence_variant': 5,\n",
    "    'synonymous_variant': 5,\n",
    "    'stop_retained_variant': 5,\n",
    "    'incomplete_terminal_codon_variant': 5,\n",
    "    'splice_region_variant': 5,\n",
    "    'protein_altering_variant': 10,\n",
    "    'missense_variant': 10,\n",
    "    'inframe_deletion': 15,\n",
    "    'inframe_insertion': 15,\n",
    "    'transcript_amplification': 15,\n",
    "    'start_lost': 15,\n",
    "    'stop_lost': 15,\n",
    "    'frameshift_variant': 20,\n",
    "    'stop_gained': 20,\n",
    "    'splice_donor_variant': 20,\n",
    "    'splice_acceptor_variant': 20,\n",
    "    'transcript_ablation': 20\n",
    "}\n",
    "\n",
    "# Helper function for dataset count that was missing\n",
    "def get_dataset_count(dataset_str):\n",
    "    \"\"\"Count the number of datasets a variant appears in\"\"\"\n",
    "    if pd.isna(dataset_str) or dataset_str == '':\n",
    "        return 0\n",
    "    return len(str(dataset_str).split(','))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475dc1b2-6b66-47ac-8645-640d0384e220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the score\n",
    "def ml_sle_variant_scoring(df):\n",
    "    \"\"\"\n",
    "     version of SLE variant scoring with improved feature extraction,\n",
    "    better handling of missing data, and ensemble anomaly detection.\n",
    "    \"\"\"\n",
    "    # Initialize features list\n",
    "    features = []\n",
    "    missing_data_flags = []  # Track which variants have missing prediction data\n",
    "    feature_names = []  # Track feature names for interpretability\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # Track missing prediction data - expanded to track partial missing data\n",
    "        missing_polyphen = pd.isna(row['PolyPhen'])\n",
    "        missing_sift = pd.isna(row['SIFT'])\n",
    "        missing_cadd = pd.isna(row['CADD_PHRED'])\n",
    "        missing_clinpred = pd.isna(row['ClinPred'])\n",
    "        \n",
    "        # Count how many prediction tools are missing\n",
    "        missing_count = sum([missing_polyphen, missing_sift, missing_cadd, missing_clinpred])\n",
    "        missing_data_flags.append(missing_count / 4.0)  # Normalized missing data ratio\n",
    "        \n",
    "        # Split consequences into list and get unique values\n",
    "        consequences = list(set(str(row['Consequence']).split(',')))\n",
    "        \n",
    "        # Get base score (maximum weight among consequences)\n",
    "        base_score = max(weights.get(consequence.strip(), 0) for consequence in consequences)\n",
    "        \n",
    "        #  Consider the top 3 consequences instead of just the maximum\n",
    "        sorted_weights = sorted([weights.get(consequence.strip(), 0) for consequence in consequences], \n",
    "                              reverse=True)\n",
    "        top_consequences_score = sum(sorted_weights[:min(3, len(sorted_weights))]) / 3\n",
    "        \n",
    "        # Functional impact scores with improved defaults and handling\n",
    "        # PolyPhen - higher is more deleterious\n",
    "        if pd.notna(row['PolyPhen']):\n",
    "            polyphen_score = max(0.1, float(row['PolyPhen']))  # Ensure non-zero values\n",
    "        else:\n",
    "            polyphen_score = 0.4  # Moderate default\n",
    "        \n",
    "        # SIFT - lower is more deleterious, so invert\n",
    "        if pd.notna(row['SIFT']):\n",
    "            sift_score = max(0.001, float(row['SIFT']))  # Ensure non-zero values\n",
    "            sift_score = 1 - sift_score  # Invert so higher is more deleterious\n",
    "        else:\n",
    "            sift_score = 0.6  # Moderate default after inversion\n",
    "            \n",
    "        # ClinPred - higher is more pathogenic\n",
    "        if pd.notna(row['ClinPred']):\n",
    "            clinpred = max(0.1, float(row['ClinPred']))  # Ensure non-zero values\n",
    "        else:\n",
    "            clinpred = 0.5  # Moderate default\n",
    "\n",
    "        # CADD\n",
    "        cadd_phred = float(row['CADD_PHRED']) if pd.notna(row['CADD_PHRED']) else 0\n",
    "        if cadd_phred >= 20:\n",
    "            base_score += 10\n",
    "        elif cadd_phred >= 10:\n",
    "            base_score += 5\n",
    "        # CADD - higher is more deleterious\n",
    "        if pd.notna(row['CADD_PHRED']):\n",
    "            cadd = max(1.0, float(row['CADD_PHRED']))  # Ensure non-zero values\n",
    "        else:\n",
    "            cadd = 15  # Moderate default\n",
    "        \n",
    "        #  Create a composite pathogenicity score\n",
    "        normalized_cadd = min(1, cadd / 35)  # Normalize CADD (increased max from 30 to 35)\n",
    "       \"\"\" # Give CADD double weight compared to others\n",
    "        cadd_weight = 0.4  # 40% weight to CADD\n",
    "        other_weight = 0.2  # 20% weight each to other predictors (total 60%)\n",
    "        \n",
    "        pathogenicity_composite = (\n",
    "            normalized_cadd * cadd_weight +\n",
    "            polyphen_score * other_weight + \n",
    "            sift_score * other_weight + \n",
    "            clinpred * other_weight\n",
    "        )\"\"\"\n",
    "       pathogenicity_composite = (polyphen_score + sift_score + clinpred + normalized_cadd) / 4\n",
    "        \n",
    "        # SLE-specific features - critical for disease association\n",
    "        healthy_presence = float(row['Healthy']) if pd.notna(row['Healthy']) else 0\n",
    "        sle_presence = float(row['SLE']) if pd.notna(row['SLE']) else 0\n",
    "        \n",
    "        #  More nuanced ratio calculation\n",
    "        if healthy_presence == 0:\n",
    "            if sle_presence > 0:\n",
    "                sle_healthy_ratio = 10  # SLE-only variants\n",
    "            else:\n",
    "                sle_healthy_ratio = 0   # No presence in either group\n",
    "        else:\n",
    "            # Log-scaled ratio to better handle outliers\n",
    "            ratio = sle_presence / (healthy_presence + 0.001)\n",
    "            sle_healthy_ratio = min(10, np.log1p(ratio) * 3)\n",
    "            \n",
    "        #  Disease enrichment score\n",
    "        disease_enrichment = np.log1p(sle_presence) - np.log1p(healthy_presence)\n",
    "        \n",
    "        # Frequency metrics with improved normalization\n",
    "        dataset_coverage = get_dataset_count(row['Datasets'])\n",
    "        normalized_dataset_coverage = min(1.0, dataset_coverage / 5)  # Cap at 5 datasets\n",
    "        \n",
    "        max_occurrence = df['Occurrence'].max() if 'Occurrence' in df.columns else 1\n",
    "        normalized_occurrence = float(row['Occurrence']) / max_occurrence if max_occurrence > 0 else 0\n",
    "        \n",
    "        #  Rarity score with log-scaling for better sensitivity\n",
    "        af = float(row['AF']) if pd.notna(row['AF']) else 0\n",
    "        if af == 0:\n",
    "            rarity_score = 1.0  # Maximum rarity for zero frequency\n",
    "        else:\n",
    "            # Log-transformed AF with stronger penalty for common variants\n",
    "            rarity_score = max(0, 1 - (np.log1p(af * 1000) / np.log1p(1000)))\n",
    "            # Extra boost for very rare variants\n",
    "            if af < 0.001:\n",
    "                rarity_score *= 1.5\n",
    "        \n",
    "        # Impact score with confidence weighting\n",
    "        impact_score = 0.5  # Default for LOW or missing\n",
    "        if pd.notna(row['IMPACT']):\n",
    "            if row['IMPACT'] == 'HIGH':\n",
    "                impact_score = 1.8  # Increased weight\n",
    "            elif row['IMPACT'] == 'MODERATE':\n",
    "                impact_score = 1.2\n",
    "        \n",
    "        # Confidence score\n",
    "        confidence = float(row['confidence']) if pd.notna(row['confidence']) else 0\n",
    "        \n",
    "        #  Check for protein structural changes\n",
    "        aa_change = False\n",
    "        severe_aa_change = False\n",
    "        \n",
    "        if 'Amino_acids' in row and pd.notna(row['Amino_acids']):\n",
    "            aa_str = str(row['Amino_acids'])\n",
    "            if (aa_str != \"0\" and aa_str.strip() != \"\" and not aa_str.endswith(\"=\") and aa_str != \"-\"):\n",
    "                aa_change = True\n",
    "                \n",
    "                # Check for chemically significant amino acid changes if format is like \"A/B\"\n",
    "                if '/' in aa_str:\n",
    "                    aa_parts = aa_str.split('/')\n",
    "                    if len(aa_parts) == 2:\n",
    "                        # Define amino acid property groups\n",
    "                        hydrophobic = set(['A', 'I', 'L', 'M', 'F', 'V', 'P', 'G'])\n",
    "                        polar = set(['Q', 'N', 'H', 'S', 'T', 'Y', 'C', 'W'])\n",
    "                        charged_positive = set(['K', 'R'])\n",
    "                        charged_negative = set(['D', 'E'])\n",
    "                        \n",
    "                        def get_aa_group(aa):\n",
    "                            if aa in hydrophobic: return 'hydrophobic'\n",
    "                            if aa in polar: return 'polar'\n",
    "                            if aa in charged_positive: return 'positive'\n",
    "                            if aa in charged_negative: return 'negative'\n",
    "                            return 'unknown'\n",
    "                        \n",
    "                        orig = aa_parts[0][0] if len(aa_parts[0]) > 0 else ''\n",
    "                        new = aa_parts[1][0] if len(aa_parts[1]) > 0 else ''\n",
    "                        \n",
    "                        if orig and new and orig != new:\n",
    "                            orig_group = get_aa_group(orig)\n",
    "                            new_group = get_aa_group(new)\n",
    "                            \n",
    "                            # Changes between property groups are more severe\n",
    "                            if orig_group != new_group and orig_group != 'unknown' and new_group != 'unknown':\n",
    "                                severe_aa_change = True\n",
    "        \n",
    "        # Apply amino acid change factor\n",
    "        aa_factor = 2.5 if severe_aa_change else (1.8 if aa_change else 1.0)\n",
    "        \n",
    "        #  Consider functional importance with more nuance\n",
    "        functional_impact = (polyphen_score >= 0.447 or sift_score >= 0.95 or normalized_cadd >= 0.7 or clinpred >= 0.7)\n",
    "        likely_functional = (polyphen_score >= 0.2 or sift_score >= 0.6 or normalized_cadd >= 0.5 or clinpred >= 0.5)\n",
    "        \n",
    "        function_score = 2.0 if functional_impact else (1.5 if likely_functional else 1.0)\n",
    "        \n",
    "        #  Create more nuanced feature vector\n",
    "        variant_features = [\n",
    "            sle_presence * 2.5,                        # Increased SLE presence weight\n",
    "            -healthy_presence * 1.2,                   # Increased penalty for healthy presence\n",
    "            sle_healthy_ratio * 0.4,                   # SLE/healthy ratio\n",
    "            disease_enrichment * 1.5,                  # Disease enrichment score\n",
    "            normalized_occurrence * 0.8,               # Moderated occurrence weight\n",
    "            normalized_dataset_coverage * 0.7,         # Dataset coverage\n",
    "            rarity_score * 1.5,                        # Rarity importance\n",
    "            confidence * 0.8,                          # Confidence score\n",
    "            clinpred * 1.2 * aa_factor,                # ClinPred boosted by aa changes\n",
    "            polyphen_score * 1.2 * aa_factor,          # PolyPhen boosted by aa changes\n",
    "            sift_score * 1.2 * aa_factor,              # SIFT boosted by aa changes\n",
    "            normalized_cadd * 1.2 * aa_factor,         # CADD boosted by aa changes\n",
    "            pathogenicity_composite * 1.8,             # Composite pathogenicity\n",
    "            impact_score * 1.5,                        # Increased impact weight\n",
    "            aa_factor * 1.2,                           # Amino acid change factor\n",
    "            function_score * 1.5,                      # Functional importance\n",
    "            base_score * 0.4,                          # Base consequence score\n",
    "            top_consequences_score * 0.5,              # Top consequences average\n",
    "        ]\n",
    "        \n",
    "        # Track feature names for interpretability\n",
    "        if len(feature_names) == 0:\n",
    "            feature_names = [\n",
    "                \"SLE_presence\", \"Healthy_presence_penalty\", \"SLE_healthy_ratio\", \n",
    "                \"Disease_enrichment\", \"Occurrence\", \"Dataset_coverage\", \n",
    "                \"Rarity\", \"Confidence\", \"ClinPred\", \"PolyPhen\", \n",
    "                \"SIFT\", \"CADD\", \"Pathogenicity_composite\", \"Impact\", \n",
    "                \"AA_change_factor\", \"Function_score\", \"Base_consequence\", \n",
    "                \"Top_consequences_avg\"\n",
    "            ]\n",
    "        \n",
    "        features.append(variant_features)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    features = np.array(features)\n",
    "    missing_data_flags = np.array(missing_data_flags)\n",
    "    \n",
    "    #  Scale features with robust scaling for better outlier handling\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    \n",
    "    #  Apply PCA with variance explanation analysis\n",
    "    pca = PCA()\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    # Determine optimal number of components (explaining 95% variance)\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    optimal_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "    \n",
    "    # Reduce to optimal components\n",
    "    pca = PCA(n_components=optimal_components)\n",
    "    features_pca = pca.fit_transform(features_scaled)\n",
    "    \n",
    "    #  Ensemble of anomaly detection methods\n",
    "    # 1. Isolation Forest with different contamination levels\n",
    "    iforest_loose = IsolationForest(n_estimators=100, contamination=0.2, random_state=42)\n",
    "    iforest_tight = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
    "    \n",
    "    iforest_loose.fit(features_pca)\n",
    "    iforest_tight.fit(features_pca)\n",
    "    \n",
    "    iforest_loose_scores = -iforest_loose.score_samples(features_pca)\n",
    "    iforest_tight_scores = -iforest_tight.score_samples(features_pca)\n",
    "    \n",
    "    # 2. Local Outlier Factor with different neighborhood sizes\n",
    "    lof_small = LocalOutlierFactor(n_neighbors=10, contamination=0.15, novelty=True)\n",
    "    lof_large = LocalOutlierFactor(n_neighbors=20, contamination=0.15, novelty=True)\n",
    "    \n",
    "    lof_small.fit(features_pca)\n",
    "    lof_large.fit(features_pca)\n",
    "    \n",
    "    lof_small_scores = -lof_small.score_samples(features_pca)\n",
    "    lof_large_scores = -lof_large.score_samples(features_pca)\n",
    "    \n",
    "    #  Ensemble scoring with weighted combination\n",
    "    ensemble_scores = (\n",
    "        0.3 * iforest_loose_scores + \n",
    "        0.2 * iforest_tight_scores + \n",
    "        0.3 * lof_small_scores + \n",
    "        0.2 * lof_large_scores\n",
    "    )\n",
    "    \n",
    "    #  Add distance-based metrics in PCA space\n",
    "    # Distance from origin in PCA space (useful for finding extreme values)\n",
    "    pca_distances = np.sqrt(np.sum(features_pca**2, axis=1))\n",
    "    pca_distances = (pca_distances - np.min(pca_distances)) / (np.max(pca_distances) - np.min(pca_distances))\n",
    "    \n",
    "    #  Create final scoring with adaptive weighting\n",
    "    # Weight anomaly scores higher for variants with amino acid changes or functional predictions\n",
    "    aa_present = np.array([features[i, 14] > 1.2 for i in range(len(features))])\n",
    "    functional_present = np.array([features[i, 15] > 1.2 for i in range(len(features))])\n",
    "    \n",
    "    # Adjust weights based on variant characteristics\n",
    "    ensemble_weight = 0.6 + 0.1 * aa_present + 0.1 * functional_present\n",
    "    distance_weight = 1.0 - ensemble_weight\n",
    "    \n",
    "    # Blend scores with adaptive weights\n",
    "    ml_scores = ensemble_weight * ensemble_scores + distance_weight * pca_distances\n",
    "    \n",
    "    #  Apply sigmoid transformation with adaptive parameters\n",
    "    # Use steeper sigmoid for variants with more prediction data\n",
    "    sigmoid_steepness = 2.0 - missing_data_flags * 0.5  # Less steep for variants with missing data\n",
    "    sigmoid_shift = np.mean(ml_scores) * 0.9  # Shift sigmoid slightly left of mean\n",
    "    \n",
    "    # Apply individualized sigmoid to each score\n",
    "    sigmoid_scores = 1 / (1 + np.exp(-sigmoid_steepness * (ml_scores - sigmoid_shift)))\n",
    "    \n",
    "    #  Boost scores for SLE-specific variants and those with missing data\n",
    "    for i in range(len(features)):\n",
    "        boost_factor = 1.0\n",
    "        \n",
    "        # Variants present in SLE but not or rarely in healthy controls\n",
    "        if features[i, 0] > 0 and features[i, 1] > -0.5:\n",
    "            boost_factor *= 1.3  # Increased from 1.25\n",
    "        \n",
    "        # Higher boost for variants with strong SLE presence and no healthy presence\n",
    "        if features[i, 0] > 1.5 and features[i, 1] == 0:\n",
    "            boost_factor *= 1.2  # Additional multiplier\n",
    "        \n",
    "        # Variants with amino acid changes\n",
    "        if features[i, 14] > 1.5:\n",
    "            boost_factor *= 1.2  # Increased from 1.15\n",
    "        \n",
    "        # Variants with high pathogenicity composite but missing some predictions\n",
    "        if features[i, 12] > 0.7 and missing_data_flags[i] > 0.25:\n",
    "            boost_factor *= 1.15  # Increased from 1.1\n",
    "        \n",
    "        # CRITICAL:  handling for variants with missing prediction data\n",
    "        # We now use a more granular approach based on how much data is missing\n",
    "        # and what other evidence we have\n",
    "        \n",
    "        # Partial missing data (25-50%)\n",
    "        if 0.25 < missing_data_flags[i] <= 0.5:\n",
    "            # Has disease association or functional evidence\n",
    "            if features[i, 0] > 0.5 or features[i, 14] > 1.2 or features[i, 16] > 8:\n",
    "                boost_factor *= 1.15\n",
    "        \n",
    "        # Significant missing data (50-75%)\n",
    "        elif 0.5 < missing_data_flags[i] <= 0.75:\n",
    "            # Strong disease association or important consequence\n",
    "            if features[i, 0] > 1.0 or features[i, 3] > 0.5 or features[i, 16] > 10:\n",
    "                boost_factor *= 1.35\n",
    "            \n",
    "            # Has amino acid changes\n",
    "            if features[i, 14] > 1.2:\n",
    "                boost_factor *= 1.2\n",
    "            \n",
    "            # Rare variant with high impact\n",
    "            if features[i, 6] > 0.8 and features[i, 13] > 1.0:\n",
    "                boost_factor *= 1.25\n",
    "                \n",
    "            # Missing particular tools but has others\n",
    "            # If CADD is present but others missing\n",
    "            if features[i, 11] > 0.7 and missing_data_flags[i] > 0.5:\n",
    "                boost_factor *= 1.15\n",
    "            \n",
    "            # If any predictor shows high pathogenicity despite others missing\n",
    "            max_pred_score = max(features[i, 8:12])  # Max of prediction scores\n",
    "            if max_pred_score > 0.8:\n",
    "                boost_factor *= 1.2\n",
    "        \n",
    "        # Mostly or completely missing data (>75%)\n",
    "        elif missing_data_flags[i] > 0.75:\n",
    "            # Still has strong disease association\n",
    "            if features[i, 0] > 1.0 or features[i, 3] > 0.7:\n",
    "                boost_factor *= 1.45  # Increased from 1.35\n",
    "            \n",
    "            # Important consequence type\n",
    "            if features[i, 16] > 15:  # Higher threshold for more severe consequences\n",
    "                boost_factor *= 1.5\n",
    "            elif features[i, 16] > 10:\n",
    "                boost_factor *= 1.3\n",
    "            \n",
    "            # Has amino acid changes\n",
    "            if features[i, 14] > 1.5:  # Severe AA change\n",
    "                boost_factor *= 1.4  # Increased from 1.2\n",
    "            elif features[i, 14] > 1.2:  # Any AA change\n",
    "                boost_factor *= 1.25\n",
    "            \n",
    "            # Rare variant with high impact\n",
    "            if features[i, 6] > 0.9 and features[i, 13] > 1.5:  # Very rare, high impact\n",
    "                boost_factor *= 1.5  # Increased from 1.3\n",
    "            elif features[i, 6] > 0.8 and features[i, 13] > 1.0:  # Rare, moderate impact\n",
    "                boost_factor *= 1.3\n",
    "            \n",
    "            # Strong presence across multiple datasets\n",
    "            if features[i, 5] > 0.6:  # Present in multiple datasets\n",
    "                boost_factor *= 1.2\n",
    "            \n",
    "            # Extremely high confidence even with missing predictions\n",
    "            if features[i, 7] > 0.8:  # High confidence\n",
    "                boost_factor *= 1.25\n",
    "            \n",
    "            # If the variant has strong indicators despite missing almost all predictions,\n",
    "            # give it an additional boost\n",
    "            if missing_data_flags[i] > 0.9 and (\n",
    "                (features[i, 0] > 0 and features[i, 1] == 0) or  # SLE-specific\n",
    "                features[i, 16] > 15 or  # High-impact consequence\n",
    "                features[i, 14] > 1.5    # Severe AA change\n",
    "            ):\n",
    "                boost_factor *= 1.15  # Extra boost for critical variants with minimal data\n",
    "        \n",
    "        # Apply combined boost factor\n",
    "        if boost_factor > 1.0:\n",
    "            sigmoid_scores[i] = min(1.0, sigmoid_scores[i] * boost_factor)\n",
    "    \n",
    "    # Final normalization to 0-100 range\n",
    "    normalized_scores = (sigmoid_scores - np.min(sigmoid_scores)) / (np.max(sigmoid_scores) - np.min(sigmoid_scores)) * 100\n",
    "    \n",
    "    #  Apply smoothed quantile-based scaling to better distribute scores\n",
    "    percentiles = np.percentile(normalized_scores, [0, 10, 25, 50, 75, 90, 100])\n",
    "    target_percentiles = [0, 25, 40, 60, 80, 90, 100]  # Desired distribution\n",
    "    \n",
    "    # Map percentiles using piecewise linear interpolation\n",
    "    final_scores = np.zeros_like(normalized_scores)\n",
    "    for i in range(len(percentiles) - 1):\n",
    "        mask = (normalized_scores >= percentiles[i]) & (normalized_scores <= percentiles[i+1])\n",
    "        if np.any(mask):\n",
    "            final_scores[mask] = np.interp(\n",
    "                normalized_scores[mask], \n",
    "                [percentiles[i], percentiles[i+1]], \n",
    "                [target_percentiles[i], target_percentiles[i+1]]\n",
    "            )\n",
    "    \n",
    "    return final_scores, feature_names, features, pca, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bb2b24-e24d-4a01-a3bb-d92893bc47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score interpretations\n",
    "def apply_ml_scoring(df):\n",
    "    \"\"\"\n",
    "    Apply  ML-based scoring with improved interpretability\n",
    "    and return annotated dataframe with feature importance.\n",
    "    \"\"\"\n",
    "    # Make a copy of the dataframe to avoid modifying the original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Apply ML scoring with  analytics\n",
    "    scores, feature_names, features, pca, scaler = ml_sle_variant_scoring(df_copy)\n",
    "    df_copy['ml_score'] = scores\n",
    "    \n",
    "    #  Calculate feature importance using PCA loadings\n",
    "    pca_loadings = pca.components_\n",
    "    feature_importance = np.abs(pca_loadings[0, :])  # Use first principal component\n",
    "    importance_dict = dict(zip(feature_names, feature_importance))\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Top contributing features to variant scoring:\")\n",
    "    for feature, importance in sorted_features[:5]:\n",
    "        print(f\"  - {feature}: {importance:.3f}\")\n",
    "    \n",
    "    #  Add variant score explanations\n",
    "    explanations = []\n",
    "    for i, score in enumerate(scores):\n",
    "        # Get top contributing features for this variant\n",
    "        variant_features = features[i]\n",
    "        scaled_features = scaler.transform([variant_features])[0]\n",
    "        \n",
    "        # Calculate feature contributions\n",
    "        contributions = {}\n",
    "        for j, feature in enumerate(feature_names):\n",
    "            contributions[feature] = scaled_features[j] * feature_importance[j]\n",
    "        \n",
    "        # Sort contributions\n",
    "        sorted_contribs = sorted(contributions.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "        top_factors = [f\"{name} ({contrib:.2f})\" for name, contrib in sorted_contribs[:3]]\n",
    "        \n",
    "        # Generate explanation\n",
    "        if score >= 85:\n",
    "            explanation = f\"Very high risk variant. Top factors: {', '.join(top_factors)}\"\n",
    "        elif score >= 70:\n",
    "            explanation = f\"High risk variant. Top factors: {', '.join(top_factors)}\"\n",
    "        elif score >= 55:\n",
    "            explanation = f\"Moderate-high risk variant. Top factors: {', '.join(top_factors)}\"\n",
    "        elif score >= 40:\n",
    "            explanation = f\"Moderate risk variant. Top factors: {', '.join(top_factors)}\"\n",
    "        elif score >= 25:\n",
    "            explanation = f\"Low-moderate risk variant. Notable factors: {', '.join(top_factors)}\"\n",
    "        elif score >= 10:\n",
    "            explanation = f\"Low risk variant.\"\n",
    "        else:\n",
    "            explanation = f\"Very low risk variant.\"\n",
    "            \n",
    "        explanations.append(explanation)\n",
    "    \n",
    "    #  Create risk interpretations with more context\n",
    "    interpretations = []\n",
    "    for score in scores:\n",
    "        if score >= 85:\n",
    "            interpretations.append(\"Very high risk SLE-associated variant\")\n",
    "        elif score >= 70:\n",
    "            interpretations.append(\"High risk SLE-associated variant\")\n",
    "        elif score >= 55:\n",
    "            interpretations.append(\"Moderate-high risk SLE-associated variant\")\n",
    "        elif score >= 40:\n",
    "            interpretations.append(\"Moderate risk SLE-associated variant\")\n",
    "        elif score >= 25:\n",
    "            interpretations.append(\"Low-moderate risk SLE-associated variant\")\n",
    "        elif score >= 10:\n",
    "            interpretations.append(\"Low risk SLE-associated variant\")\n",
    "        else:\n",
    "            interpretations.append(\"Very low risk SLE-associated variant\")\n",
    "    \n",
    "    # Add explanations and interpretations to dataframe\n",
    "    df_copy['ml_interpretation'] = interpretations\n",
    "    df_copy['ml_explanation'] = explanations\n",
    "    \n",
    "    # Sort by score\n",
    "    df_sorted = df_copy.sort_values('ml_score', ascending=False)\n",
    "    \n",
    "    # Create a more detailed distribution analysis\n",
    "    print(\"\\nML Score Distribution:\")\n",
    "    categories = [\n",
    "        (\"Very high risk (85-100)\", lambda s: s >= 85),\n",
    "        (\"High risk (70-84)\", lambda s: 70 <= s < 85),\n",
    "        (\"Moderate-high risk (55-69)\", lambda s: 55 <= s < 70),\n",
    "        (\"Moderate risk (40-54)\", lambda s: 40 <= s < 55),\n",
    "        (\"Low-moderate risk (25-39)\", lambda s: 25 <= s < 40),\n",
    "        (\"Low risk (10-24)\", lambda s: 10 <= s < 25),\n",
    "        (\"Very low risk (0-9)\", lambda s: s < 10)\n",
    "    ]\n",
    "    \n",
    "    for category_name, condition in categories:\n",
    "        count = sum(1 for s in scores if condition(s))\n",
    "        print(f\"{category_name}: {count} variants ({count/len(scores)*100:.1f}%)\")\n",
    "    \n",
    "    return df_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32e65e-5d22-4a53-955e-1bb856072763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing the missing data\n",
    "def identify_high_risk_variants_with_missing_data(df_scored):\n",
    "    \"\"\"\n",
    "    Specifically identify high-risk variants that are missing prediction scores.\n",
    "    These variants might be overlooked by traditional filtering approaches.\n",
    "    \"\"\"\n",
    "    # Check which prediction columns exist in the dataset\n",
    "    prediction_columns = ['PolyPhen', 'SIFT', 'CADD_PHRED', 'ClinPred']\n",
    "    available_columns = [col for col in prediction_columns if col in df_scored.columns]\n",
    "    \n",
    "    if not available_columns:\n",
    "        print(\"No prediction score columns found in dataset.\")\n",
    "        return df_scored\n",
    "    \n",
    "    # Create a missing data flag\n",
    "    df_scored['missing_prediction_data'] = False\n",
    "    df_scored['missing_predictions_count'] = 0\n",
    "    \n",
    "    # Count missing prediction data for each variant\n",
    "    for col in available_columns:\n",
    "        df_scored['missing_prediction_data'] |= pd.isna(df_scored[col])\n",
    "        df_scored['missing_predictions_count'] += pd.isna(df_scored[col]).astype(int)\n",
    "    \n",
    "    # Calculate the percentage of missing prediction data\n",
    "    df_scored['missing_predictions_percent'] = (df_scored['missing_predictions_count'] / len(available_columns)) * 100\n",
    "    \n",
    "    # Add a score adjustment for variants with missing data but other strong indicators\n",
    "    # This helps identify potential high-risk variants that would be missed by standard filters\n",
    "    df_scored['missing_data_risk_adjustment'] = 0.0\n",
    "    \n",
    "    for idx, row in df_scored.iterrows():\n",
    "        if row['missing_predictions_count'] > 0:\n",
    "            adjustment = 0.0\n",
    "            adjustment_reasons = []\n",
    "            \n",
    "            # Check for strong disease association\n",
    "            if 'SLE' in df_scored.columns and 'Healthy' in df_scored.columns:\n",
    "                sle_presence = float(row['SLE']) if pd.notna(row['SLE']) else 0\n",
    "                healthy_presence = float(row['Healthy']) if pd.notna(row['Healthy']) else 0\n",
    "                \n",
    "                if sle_presence > 0 and healthy_presence == 0:\n",
    "                    adjustment += 15.0  # SLE-specific variants\n",
    "                    adjustment_reasons.append(\"SLE-specific\")\n",
    "                elif sle_presence > healthy_presence * 2:\n",
    "                    adjustment += 10.0  # SLE-enriched variants\n",
    "                    adjustment_reasons.append(\"SLE-enriched\")\n",
    "            \n",
    "            # Check for important structural consequences\n",
    "            if 'Consequence' in df_scored.columns and pd.notna(row['Consequence']):\n",
    "                consequences = str(row['Consequence']).split(',')\n",
    "                max_weight = max(weights.get(consequence.strip(), 0) for consequence in consequences)\n",
    "                \n",
    "                if max_weight >= 15:  # High-impact consequences\n",
    "                    adjustment += 12.0\n",
    "                    adjustment_reasons.append(\"High-impact consequence\")\n",
    "                elif max_weight >= 10:  # Moderate-impact consequences\n",
    "                    adjustment += 8.0\n",
    "                    adjustment_reasons.append(\"Moderate-impact consequence\")\n",
    "            \n",
    "            # Check for amino acid changes\n",
    "            if 'Amino_acids' in df_scored.columns and pd.notna(row['Amino_acids']):\n",
    "                aa_str = str(row['Amino_acids'])\n",
    "                if (aa_str != \"0\" and aa_str.strip() != \"\" and not aa_str.endswith(\"=\") and aa_str != \"-\"):\n",
    "                    adjustment += 10.0\n",
    "                    adjustment_reasons.append(\"Amino acid change\")\n",
    "                    \n",
    "                    # Check for significant property changes\n",
    "                    if '/' in aa_str:\n",
    "                        aa_parts = aa_str.split('/')\n",
    "                        if len(aa_parts) == 2:\n",
    "                            # Define amino acid property groups\n",
    "                            hydrophobic = set(['A', 'I', 'L', 'M', 'F', 'V', 'P', 'G'])\n",
    "                            polar = set(['Q', 'N', 'H', 'S', 'T', 'Y', 'C', 'W'])\n",
    "                            charged_positive = set(['K', 'R'])\n",
    "                            charged_negative = set(['D', 'E'])\n",
    "                            \n",
    "                            def get_aa_group(aa):\n",
    "                                if aa in hydrophobic: return 'hydrophobic'\n",
    "                                if aa in polar: return 'polar'\n",
    "                                if aa in charged_positive: return 'positive'\n",
    "                                if aa in charged_negative: return 'negative'\n",
    "                                return 'unknown'\n",
    "                            \n",
    "                            orig = aa_parts[0][0] if len(aa_parts[0]) > 0 else ''\n",
    "                            new = aa_parts[1][0] if len(aa_parts[1]) > 0 else ''\n",
    "                            \n",
    "                            if orig and new and orig != new:\n",
    "                                orig_group = get_aa_group(orig)\n",
    "                                new_group = get_aa_group(new)\n",
    "                                \n",
    "                                # Changes between property groups are more severe\n",
    "                                if orig_group != new_group and orig_group != 'unknown' and new_group != 'unknown':\n",
    "                                    adjustment += 5.0\n",
    "                                    adjustment_reasons.append(\"Property-changing AA substitution\")\n",
    "            \n",
    "            # Check for rarity - rare variants with missing data might be important\n",
    "            if 'AF' in df_scored.columns:\n",
    "                af = float(row['AF']) if pd.notna(row['AF']) else 0\n",
    "                if af == 0:\n",
    "                    adjustment += 8.0  # Unique variants\n",
    "                    adjustment_reasons.append(\"Unique variant\")\n",
    "                elif af < 0.001:\n",
    "                    adjustment += 5.0  # Very rare variants\n",
    "                    adjustment_reasons.append(\"Very rare variant\")\n",
    "            \n",
    "            # Check for high impact\n",
    "            if 'IMPACT' in df_scored.columns and pd.notna(row['IMPACT']):\n",
    "                if row['IMPACT'] == 'HIGH':\n",
    "                    adjustment += 15.0\n",
    "                    adjustment_reasons.append(\"HIGH impact\")\n",
    "                elif row['IMPACT'] == 'MODERATE':\n",
    "                    adjustment += 8.0\n",
    "                    adjustment_reasons.append(\"MODERATE impact\")\n",
    "            \n",
    "            # Apply the adjustment, scaled by how much data is missing\n",
    "            # More missing data = higher potential adjustment\n",
    "            missing_ratio = row['missing_predictions_count'] / len(available_columns)\n",
    "            final_adjustment = adjustment * missing_ratio\n",
    "            \n",
    "            # Only apply significant adjustments\n",
    "            if final_adjustment >= 5.0:\n",
    "                df_scored.at[idx, 'missing_data_risk_adjustment'] = final_adjustment\n",
    "                \n",
    "                # Also create an explanation field if it doesn't exist\n",
    "                if 'missing_data_explanation' not in df_scored.columns:\n",
    "                    df_scored['missing_data_explanation'] = \"\"\n",
    "                \n",
    "                # Add explanation\n",
    "                df_scored.at[idx, 'missing_data_explanation'] = f\"Missing data risk factors: {', '.join(adjustment_reasons)}\"\n",
    "    \n",
    "    # Apply the adjustment to create a new adjusted score\n",
    "    # but only for variants with missing data\n",
    "    df_scored['ml_score_adjusted'] = df_scored['ml_score']\n",
    "    mask = df_scored['missing_predictions_count'] > 0\n",
    "    df_scored.loc[mask, 'ml_score_adjusted'] = df_scored.loc[mask, 'ml_score'] + df_scored.loc[mask, 'missing_data_risk_adjustment']\n",
    "    \n",
    "    # Cap the adjusted score at 100\n",
    "    df_scored['ml_score_adjusted'] = df_scored['ml_score_adjusted'].clip(upper=100)\n",
    "    \n",
    "    # Create new interpretation based on adjusted score\n",
    "    df_scored['ml_interpretation_adjusted'] = df_scored['ml_interpretation']\n",
    "    \n",
    "    # Update interpretations for adjusted scores\n",
    "    for idx, row in df_scored.iterrows():\n",
    "        if row['missing_predictions_count'] > 0 and row['missing_data_risk_adjustment'] > 0:\n",
    "            adjusted_score = row['ml_score_adjusted']\n",
    "            \n",
    "            if adjusted_score >= 85:\n",
    "                df_scored.at[idx, 'ml_interpretation_adjusted'] = \"Very high risk SLE-associated variant (adjusted for missing data)\"\n",
    "            elif adjusted_score >= 70:\n",
    "                df_scored.at[idx, 'ml_interpretation_adjusted'] = \"High risk SLE-associated variant (adjusted for missing data)\"\n",
    "            elif adjusted_score >= 55:\n",
    "                df_scored.at[idx, 'ml_interpretation_adjusted'] = \"Moderate-high risk SLE-associated variant (adjusted for missing data)\"\n",
    "            elif adjusted_score >= 40:\n",
    "                df_scored.at[idx, 'ml_interpretation_adjusted'] = \"Moderate risk SLE-associated variant (adjusted for missing data)\"\n",
    "    \n",
    "    # Identify potentially high-risk variants with missing data\n",
    "    high_risk_missing = df_scored[\n",
    "        (df_scored['missing_predictions_count'] > 0) & \n",
    "        (df_scored['ml_score_adjusted'] >= 40)  # Moderate or higher adjusted risk score\n",
    "    ].copy()\n",
    "    \n",
    "    # Sort by adjusted risk score\n",
    "    high_risk_missing = high_risk_missing.sort_values('ml_score_adjusted', ascending=False)\n",
    "    \n",
    "    # Create a flag for variants that were promoted to higher risk categories\n",
    "    df_scored['risk_category_promoted'] = False\n",
    "    \n",
    "    for idx, row in df_scored.iterrows():\n",
    "        if row['missing_predictions_count'] > 0:\n",
    "            original_score = row['ml_score']\n",
    "            adjusted_score = row['ml_score_adjusted']\n",
    "            \n",
    "            # Check if variant moved to a higher risk category\n",
    "            if (original_score < 85 and adjusted_score >= 85) or \\\n",
    "               (original_score < 70 and adjusted_score >= 70) or \\\n",
    "               (original_score < 55 and adjusted_score >= 55) or \\\n",
    "               (original_score < 40 and adjusted_score >= 40):\n",
    "                df_scored.at[idx, 'risk_category_promoted'] = True\n",
    "    \n",
    "    # Identify newly discovered high-risk variants\n",
    "    promoted_variants = df_scored[df_scored['risk_category_promoted'] == True].sort_values('ml_score_adjusted', ascending=False)\n",
    "    \n",
    "    return df_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a6ad9-e116-4d7c-97c1-5d5ff94a7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Apply the ML scoring\n",
    "ml_scored_variants = apply_ml_scoring(result_df)\n",
    "updated_scored_variants = Final_table_unfiltered.copy()\n",
    "\n",
    "# Step 2: Identify variants with missing prediction data that may still be high-risk\n",
    "variants_with_adjusted_scores = identify_high_risk_variants_with_missing_data(ml_scored_variants)\n",
    "updated_scored_variants = updated_scored_variants.merge(\n",
    "    variants_with_adjusted_scores[['POS', 'ml_score_adjusted', 'ml_interpretation']], \n",
    "    on='POS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4469c05a-db65-4d69-9644-a22b4decbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_scored_variants = updated_scored_variants.rename(columns={'ml_score_adjusted': 'ml_score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a527e551-7d59-41d7-bc05-738c9f88c70f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#scored_variants = scored_variants.merge(Final_table_unfiltered[['POS', 'score']], on='POS')\n",
    "#cols = list(scored_variants.columns)\n",
    "#cols.insert(len(cols)-2, cols.pop(cols.index('score')))\n",
    "#scored_variants = scored_variants[cols]\n",
    "updated_scored_variants = updated_scored_variants.rename(columns={'score': 'manual_ranking_score'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "877a31cf-5676-4308-b5dd-e5147f02b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Combined Score Distribution:\n",
      "Very High: 2 variants (0.3%)\n",
      "High: 32 variants (4.8%)\n",
      "Moderate High: 166 variants (25.0%)\n",
      "Moderate: 130 variants (19.6%)\n",
      "Low Moderate: 247 variants (37.3%)\n",
      "Low: 67 variants (10.1%)\n",
      "Very Low: 19 variants (2.9%)\n"
     ]
    }
   ],
   "source": [
    "def combine_manual_and_ml_scores(df, ml_weight):\n",
    "    \"\"\"\n",
    "    Combine manual ranking score and ML score into a final score\n",
    "    \n",
    "    Parameters:\n",
    "    df: DataFrame with both 'ml_score' and 'manual_ranking_score' columns\n",
    "       (manual_ranking_score is expected to be on a 0-50 scale)\n",
    "    ml_weight: Weight for ML score (between 0 and 1), manual weight will be (1-ml_weight)\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with additional 'final_score' and 'final_score_interpretation' columns\n",
    "    \"\"\"\n",
    "    # Ensure both scores exist\n",
    "    if 'ml_score' not in df.columns or 'manual_ranking_score' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain both 'ml_score' and 'manual_ranking_score' columns\")\n",
    "    \n",
    "    # Create a copy of the dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Normalize manual score from 0-50 to 0-100 scale\n",
    "    df_copy['manual_ranking_score_normalized'] = df_copy['manual_ranking_score'] \n",
    "    \n",
    "    # Combine scores using weighted average\n",
    "    manual_weight = 1 - ml_weight\n",
    "    df_copy['final_score'] = (\n",
    "        df_copy['ml_score'] * ml_weight + \n",
    "        df_copy['manual_ranking_score_normalized'] * manual_weight\n",
    "    )\n",
    "    \n",
    "    # Create interpretations based on final score\n",
    "    interpretations = []\n",
    "    for score in df_copy['final_score']:\n",
    "        if score >= 85:\n",
    "            interpretations.append(\"Very high risk\")\n",
    "        elif score >= 70:\n",
    "            interpretations.append(\"High risk\")\n",
    "        elif score >= 55:\n",
    "            interpretations.append(\"Moderate-high risk\")\n",
    "        elif score >= 40:\n",
    "            interpretations.append(\"Moderate risk\")\n",
    "        elif score >= 25:\n",
    "            interpretations.append(\"Low-moderate risk\")\n",
    "        elif score >= 10:\n",
    "            interpretations.append(\"Low risk\")\n",
    "        else:\n",
    "            interpretations.append(\"Very low risk\")\n",
    "    \n",
    "    df_copy['final_score_interpretation'] = interpretations\n",
    "    \n",
    "    # Create a distribution analysis of the final scores\n",
    "    score_distribution = {\n",
    "        \"very_high\": sum(1 for s in df_copy['final_score'] if s >= 85),\n",
    "        \"high\": sum(1 for s in df_copy['final_score'] if 70 <= s < 85),\n",
    "        \"moderate_high\": sum(1 for s in df_copy['final_score'] if 55 <= s < 70),\n",
    "        \"moderate\": sum(1 for s in df_copy['final_score'] if 40 <= s < 55),\n",
    "        \"low_moderate\": sum(1 for s in df_copy['final_score'] if 25 <= s < 40),\n",
    "        \"low\": sum(1 for s in df_copy['final_score'] if 10 <= s < 25),\n",
    "        \"very_low\": sum(1 for s in df_copy['final_score'] if s < 10)\n",
    "    }\n",
    "    \n",
    "    # Print score distribution\n",
    "    print(\"Final Combined Score Distribution:\")\n",
    "    for category, count in score_distribution.items():\n",
    "        print(f\"{category.replace('_', ' ').title()}: {count} variants ({count/len(df_copy)*100:.1f}%)\")\n",
    "    \n",
    "    # Remove intermediate columns that are no longer needed\n",
    "    df_copy = df_copy.drop([\"manual_ranking_score_normalized\"], axis=1)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "\n",
    "#\n",
    "# Then combine with manual scores:\n",
    "final_variants = combine_manual_and_ml_scores(updated_scored_variants, ml_weight=0.5)\n",
    "#\n",
    "# To drop additional columns if needed:\n",
    "final_variants = final_variants.drop([\"ml_interpretation\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe72b7b-d866-4034-b1b6-1b582f3f3bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POS</th>\n",
       "      <th>Datasets</th>\n",
       "      <th>Healthy</th>\n",
       "      <th>SLE</th>\n",
       "      <th>Occurrence</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>GQ</th>\n",
       "      <th>QUAL</th>\n",
       "      <th>DP</th>\n",
       "      <th>...</th>\n",
       "      <th>AF</th>\n",
       "      <th>Protein_position</th>\n",
       "      <th>Amino_acids</th>\n",
       "      <th>CADD_PHRED</th>\n",
       "      <th>SIFT</th>\n",
       "      <th>PolyPhen</th>\n",
       "      <th>manual_ranking_score</th>\n",
       "      <th>ml_score</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_score_interpretation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68003029</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>99.0</td>\n",
       "      <td>875.306667</td>\n",
       "      <td>65.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>129</td>\n",
       "      <td>L129I</td>\n",
       "      <td>26.000</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.951</td>\n",
       "      <td>85.855719</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>92.927860</td>\n",
       "      <td>Very high risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67999234</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1490.640000</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>209</td>\n",
       "      <td>P209L</td>\n",
       "      <td>25.400</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.996</td>\n",
       "      <td>71.711330</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>85.855665</td>\n",
       "      <td>Very high risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67991568</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>99.0</td>\n",
       "      <td>968.640000</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>591</td>\n",
       "      <td>G591E</td>\n",
       "      <td>17.490</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.995</td>\n",
       "      <td>64.518414</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>82.259207</td>\n",
       "      <td>High risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67999680</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>50.0</td>\n",
       "      <td>42.640000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>131</td>\n",
       "      <td>R131S</td>\n",
       "      <td>25.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.350</td>\n",
       "      <td>57.554599</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>78.777300</td>\n",
       "      <td>High risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>67991757</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>569.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>528</td>\n",
       "      <td>R528H</td>\n",
       "      <td>29.200</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.891</td>\n",
       "      <td>51.914286</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>75.957143</td>\n",
       "      <td>High risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>67991207</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>178.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2.146</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.728571</td>\n",
       "      <td>0.335469</td>\n",
       "      <td>7.032020</td>\n",
       "      <td>Very low risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>67997644</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2.161</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.030874</td>\n",
       "      <td>1.733237</td>\n",
       "      <td>6.882056</td>\n",
       "      <td>Very low risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>67991322</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>308.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1.459</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.728571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.864286</td>\n",
       "      <td>Very low risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>67996064</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>G</td>\n",
       "      <td>T</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>2.950</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.031408</td>\n",
       "      <td>0.013252</td>\n",
       "      <td>6.022330</td>\n",
       "      <td>Very low risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>67997286</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>3.668</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>12.023678</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>6.012203</td>\n",
       "      <td>Very low risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>663 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          POS Datasets  Healthy  SLE  Occurrence REF ALT    GQ         QUAL  \\\n",
       "0    68003029        2        0    3           3   G   T  99.0   875.306667   \n",
       "1    67999234        2        0    3           3   G   A  99.0  1490.640000   \n",
       "2    67991568        2        0    1           1   C   T  99.0   968.640000   \n",
       "3    67999680        2        0    1           1   C   A  50.0    42.640000   \n",
       "18   67991757        3        0    1           1   C   T  99.0     0.000000   \n",
       "..        ...      ...      ...  ...         ...  ..  ..   ...          ...   \n",
       "610  67991207        3        1    0           1   C   G  99.0     0.000000   \n",
       "623  67997644        3        1    0           1   C   A  99.0     0.000000   \n",
       "596  67991322        3        1    0           1   C   G  99.0     0.000000   \n",
       "622  67996064        3        1    0           1   G   T  99.0     0.000000   \n",
       "625  67997286        3        1    0           1   C   A  99.0     0.000000   \n",
       "\n",
       "        DP  ...      AF  Protein_position  Amino_acids  CADD_PHRED  SIFT  \\\n",
       "0     65.0  ...  0.0028               129        L129I      26.000  0.02   \n",
       "1     72.0  ...  0.0028               209        P209L      25.400  0.00   \n",
       "2     41.0  ...  0.0024               591        G591E      17.490  0.48   \n",
       "3     50.0  ...  0.0000               131        R131S      25.000  0.00   \n",
       "18   569.0  ...  0.0000               528        R528H      29.200  0.04   \n",
       "..     ...  ...     ...               ...          ...         ...   ...   \n",
       "610  178.0  ...  0.0000                 -            -       2.146  0.00   \n",
       "623   60.0  ...  0.0000                 -            -       2.161  0.00   \n",
       "596  308.0  ...  0.0000                 -            -       1.459  0.00   \n",
       "622   75.0  ...  0.0000                 -            -       2.950  0.00   \n",
       "625   48.0  ...  0.0000                 -            -       3.668  0.00   \n",
       "\n",
       "    PolyPhen manual_ranking_score    ml_score final_score  \\\n",
       "0      0.951            85.855719  100.000000   92.927860   \n",
       "1      0.996            71.711330  100.000000   85.855665   \n",
       "2      0.995            64.518414  100.000000   82.259207   \n",
       "3      0.350            57.554599  100.000000   78.777300   \n",
       "18     0.891            51.914286  100.000000   75.957143   \n",
       "..       ...                  ...         ...         ...   \n",
       "610    0.000            13.728571    0.335469    7.032020   \n",
       "623    0.000            12.030874    1.733237    6.882056   \n",
       "596    0.000            13.728571    0.000000    6.864286   \n",
       "622    0.000            12.031408    0.013252    6.022330   \n",
       "625    0.000            12.023678    0.000727    6.012203   \n",
       "\n",
       "     final_score_interpretation  \n",
       "0                Very high risk  \n",
       "1                Very high risk  \n",
       "2                     High risk  \n",
       "3                     High risk  \n",
       "18                    High risk  \n",
       "..                          ...  \n",
       "610               Very low risk  \n",
       "623               Very low risk  \n",
       "596               Very low risk  \n",
       "622               Very low risk  \n",
       "625               Very low risk  \n",
       "\n",
       "[663 rows x 29 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort in descending order (highest score first)\n",
    "df_final_sorted = final_variants.sort_values('final_score', ascending=False)\n",
    "df_final_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e002b78-dc0f-431b-b9a5-3d29ccfd3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6be08c6-6be0-452d-9f7f-2072a073f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_sorted.to_csv(\"Combined_ranking_final_table_unfiltered_the_FINAL.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
