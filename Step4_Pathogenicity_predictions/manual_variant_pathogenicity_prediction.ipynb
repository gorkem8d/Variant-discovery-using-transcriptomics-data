{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d8934c-2ffd-4856-ad40-567536e1b5ed",
   "metadata": {},
   "source": [
    "## Manual variant pathogenicity predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a21ac-927d-42e6-a0dc-aac3fb8386c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6d0c30-4eeb-467c-8189-041dfbf8db6c",
   "metadata": {},
   "source": [
    "Part 1 - Common Mutations Between Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcc6d3-940c-4b68-9eb7-db3a1e941702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine datasets \n",
    "datasets = []\n",
    "for i in [1, 2, 3]:\n",
    "    datasets.append(pd.read_csv(f\"/work/project/ext_016/RNA-Seq-Variant-Calling_1/dataset_{i}_positions_confidence.csv\"))\n",
    "   \n",
    "for idx, df in enumerate(datasets, start=1):\n",
    "    df['Dataset'] = idx\n",
    "\n",
    "combined_df = pd.concat(datasets)\n",
    "\n",
    "# Standardize the labels\n",
    "combined_df['Disease'] = combined_df['Disease'].replace({\n",
    "    'Healthy': 'healthy',\n",
    "    'systemic lupus erythematosus (SLE)': 'SLE',\n",
    "    'systemic lupus erythematosus': 'SLE'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67496ff9-b4cb-489f-814c-fa0d926f09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by POS and calculate means for numeric columns\n",
    "# For other columns (REF, ALT, etc.), take the first value since they're constant\n",
    "aggregated_df = combined_df.groupby('POS').agg({\n",
    "    'Run': 'first',\n",
    "    'REF': 'first',\n",
    "    'ALT': 'first',\n",
    "    'GQ': 'mean',\n",
    "    'QUAL': 'mean',\n",
    "    'DP': 'mean',\n",
    "    'gq_conf': 'mean',\n",
    "    'qual_conf': 'mean',\n",
    "    'dp_conf': 'mean',\n",
    "    'confidence': 'mean',\n",
    "    'FILTER': 'first'\n",
    "}).reset_index()\n",
    "# aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b3d1a9-2256-46db-81c8-7bc7c5a5cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_df = []\n",
    "\n",
    "# Combine same variations and their quantitative data; Occurence in datasets, Names of the datasets, % healthy, % SLE\n",
    "pos_occurrence_counts = combined_df['POS'].value_counts().reset_index()\n",
    "pos_occurrence_counts.columns = ['POS', 'Occurrence']\n",
    "\n",
    "# Group by POS and create the aggregated dataset information\n",
    "pos_grouped = combined_df.groupby('POS').agg({\n",
    "    'Dataset': lambda x: ','.join(map(str, sorted(x.unique()))),  # Convert integers to strings before joining\n",
    "}).rename(columns={'Dataset': 'Datasets'})\n",
    "\n",
    "# Count total healthy and SLE samples\n",
    "total_healthy = len(combined_df[combined_df['Disease'] == 'healthy'].drop_duplicates('Run'))\n",
    "print(total_healthy)\n",
    "total_sle = len(combined_df[combined_df['Disease'] == 'SLE'].drop_duplicates('Run'))\n",
    "print(total_sle)\n",
    "\n",
    "# calculate percentages for Healthy and SLE\n",
    "def calculate_percentages(pos):\n",
    "    # Get all samples for this position\n",
    "    samples = combined_df[combined_df['POS'] == pos]\n",
    "    \n",
    "\n",
    "    # Count healthy and SLE samples for this position\n",
    "    pos_healthy = len(samples[samples['Disease'] == 'healthy'])\n",
    "    pos_sle = len(samples[samples['Disease'] == 'SLE'])\n",
    "    \n",
    "    # Calculate # of samples\n",
    "    healthy = pos_healthy\n",
    "    sle= pos_sle\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Healthy': round(healthy, 1),\n",
    "        'SLE': round(sle, 1)\n",
    "    })\n",
    "\n",
    "# Calculate percentages for each position\n",
    "percentages = pd.DataFrame([calculate_percentages(pos) for pos in pos_grouped.index], \n",
    "                         index=pos_grouped.index)\n",
    "\n",
    "# Combine the results\n",
    "result_df = pd.concat([pos_grouped, percentages], axis=1)\n",
    "# Replace \",<NON_REF>\" with an empty string\n",
    "result_df[\"ALT\"] = result_df[\"ALT\"].str.replace(\",<NON_REF>\", \"\", regex=False)\n",
    "# Remove the Run column\n",
    "result_df = result_df.drop(\"Run\", axis=1)\n",
    "\n",
    "# Sort by position\n",
    "result_df = result_df.sort_index()\n",
    "result_df = result_df.merge(pos_occurrence_counts, on='POS')\n",
    "result_df = result_df.merge(aggregated_df, on='POS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d2db10-4e65-414c-8703-546ac2fe33ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "vep = pd.read_csv('VEP_results_aggregated.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b6bd8-9129-42db-bdec-3879080e5c83",
   "metadata": {},
   "source": [
    "VEP Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b743f7b-1b8e-490d-a6e8-ce961675baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_df = {}\n",
    "result_df = result_df.merge(vep, on='POS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa482ac-6ce2-4140-9428-7eb21391711e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.to_csv(\"result_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53badf5-3847-4170-9a53-2363f0bb3675",
   "metadata": {},
   "source": [
    "Ordering the positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bd92e-be4f-4465-a1ce-6b100febc1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results=[]\n",
    "def calculate_vep_score(row, weights):\n",
    "    # Original score calculation\n",
    "    consequences = str(row['Consequence']).split(',')\n",
    "    base_score = max(weights.get(consequence.strip(), 0) for consequence in consequences)\n",
    "    \n",
    "    # PolyPhen\n",
    "    polyphen_score = float(row['PolyPhen']) if pd.notna(row['PolyPhen']) else 0\n",
    "    if polyphen_score >= 0.447:\n",
    "        base_score += 10\n",
    "    \n",
    "    # CADD\n",
    "    cadd_phred = float(row['CADD_PHRED']) if pd.notna(row['CADD_PHRED']) else 0\n",
    "    if cadd_phred >= 20:\n",
    "        base_score += 10\n",
    "    elif cadd_phred >= 10:\n",
    "        base_score += 5\n",
    "    \n",
    "    # SIFT\n",
    "    sift_score = float(row['SIFT']) if pd.notna(row['SIFT']) else 0\n",
    "    if sift_score <= 0.05 and sift_score > 0.00:\n",
    "        base_score += 10\n",
    "    \n",
    "    # AF\n",
    "    af_score = float(row['AF']) if pd.notna(row['AF']) else 0\n",
    "    if af_score < 0.01:\n",
    "        base_score += 10\n",
    "    \n",
    "    # SLE vs Healthy\n",
    "    lupus_score = float(row[\"SLE\"])\n",
    "    healthy_score = float(row[\"Healthy\"])\n",
    "    if lupus_score >= 1 and healthy_score == 0:\n",
    "        base_score += 10\n",
    "    \n",
    "    # Apply confidence modifier\n",
    "    base_score = base_score * row[\"confidence\"]\n",
    "    \n",
    "    # Normalize to 1-100 scale\n",
    "    # Assuming theoretical max is 70 Ã— max_confidence\n",
    "    max_possible_score = 70 \n",
    "    \n",
    "    # Scale to 1-100 (with 1 as minimum)\n",
    "    normalized_score = 1 + (base_score / max_possible_score) * 99\n",
    "    \n",
    "    # Ensure the score doesn't exceed 100\n",
    "    return min(100, normalized_score)\n",
    "\n",
    "def process_variants(df):\n",
    "    \"\"\"\n",
    "    Process all variants in a DataFrame and calculate their VEP scores.\n",
    "    \n",
    "    Parameters:\n",
    "    df: pandas DataFrame containing variant information\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame: Original DataFrame with additional 'vep_score' column\n",
    "    \"\"\"\n",
    "    # Define weights for each consequence type\n",
    "    weights = {\n",
    "        'intergenic_variant': 1,\n",
    "        'feature_truncation': 3,\n",
    "        'regulatory_region_variant': 3,\n",
    "        'feature_elongation': 3,\n",
    "        'regulatory_region_amplification': 3,\n",
    "        'regulatory_region_ablation': 3,\n",
    "        'TF_binding_site_variant': 3,\n",
    "        'TFBS_amplification': 3,\n",
    "        'TFBS_ablation': 3,\n",
    "        'downstream_gene_variant': 3,\n",
    "        'upstream_gene_variant': 3,\n",
    "        'non_coding_transcript_variant': 3,\n",
    "        'NMD_transcript_variant': 3,\n",
    "        'intron_variant': 3,\n",
    "        'non_coding_transcript_exon_variant': 3,\n",
    "        '3_prime_UTR_variant': 5,\n",
    "        '5_prime_UTR_variant': 5,\n",
    "        'mature_miRNA_variant': 5,\n",
    "        'coding_sequence_variant': 5,\n",
    "        'synonymous_variant': 5,\n",
    "        'stop_retained_variant': 5,\n",
    "        'incomplete_terminal_codon_variant': 5,\n",
    "        'splice_region_variant': 5,\n",
    "        'protein_altering_variant': 10,\n",
    "        'missense_variant': 10,\n",
    "        'inframe_deletion': 15,\n",
    "        'inframe_insertion': 15,\n",
    "        'transcript_amplification': 15,\n",
    "        'start_lost': 15,\n",
    "        'stop_lost': 15,\n",
    "        'frameshift_variant': 20,\n",
    "        'stop_gained': 20,\n",
    "        'splice_donor_variant': 20,\n",
    "        'splice_acceptor_variant': 20,\n",
    "        'transcript_ablation': 20\n",
    "    }\n",
    "    \n",
    "    # Calculate scores for all variants\n",
    "    df['score'] = df.apply(lambda row: calculate_vep_score(row, weights), axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "results = process_variants(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d55abb-6e4d-4282-88c3-403d09182a71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop the columns that are not needed\n",
    "results = results.sort_values(by='score', ascending=False)\n",
    "results= results.drop(\"CLIN_SIG\", axis = 1) \n",
    "results= results.drop(\"ClinPred\", axis = 1)\n",
    "#results= results.drop(\"CADD_PHRED\", axis = 1)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e967a5-0dff-4d83-8df8-f59c22e2fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "results.to_csv(\"Final_table_unfiltered.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
